{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "# Plot normal distribution areas*\n",
    "\n",
    "k=3 #Will plot areas below -k, above k and between -k and k\n",
    "mean=0 #plotting will assume mean=0\n",
    "std=1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (35,35)\n",
    "                                  \n",
    "plt.fill_between(x=np.arange(-4*std+mean,-k*std+mean,0.01), \n",
    "                 y1= norm.pdf(np.arange(-4*std+mean,-k*std+mean,0.01),mean,std) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(k*std+mean,k*4+mean,0.01), \n",
    "                 y1= norm.pdf(np.arange(k*std+mean,k*4+mean,0.01),mean,std) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(-k*std+mean,k*std+mean,0.01), \n",
    "                 y1= norm.pdf(np.arange(-k*std+mean,k*std+mean,0.01),mean,std) ,\n",
    "                 facecolor='blue',\n",
    "                 alpha=0.35)\n",
    "prob_under_minusk = norm.cdf(x= -k,  \n",
    "                                loc = 0,               \n",
    "                                scale= 1)     \n",
    "\n",
    "prob_over_k = 1 - norm.cdf(x= k,  \n",
    "                                loc = 0,               \n",
    "                                scale= 1) \n",
    "\n",
    "between_prob = 1-(prob_under_minusk+prob_over_k)\n",
    "plt.text(x=-1.8, y=0.03, s= round(prob_under_minusk,3))\n",
    "plt.text(x=-0.2, y=0.1, s= round(between_prob,3))\n",
    "plt.text(x=1.4, y=0.03, s= round(prob_over_k,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# <nbformat>2</nbformat>\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h1>Readings</h1>\n",
    "# <ul>\n",
    "#     <li>Bishop: 3.1.0-3.1.4</li>\n",
    "#     <li>Ng: Lecture 2 pdf, page 4, LMS algorithm</li>\n",
    "#     <li>Ng: Lecture 2 pdf, page 13, Locally weighted linear regression</li>\n",
    "#     <li>Bishop: 3.3.0-3.3.2</li>\n",
    "# </ul>\n",
    "# <p><font color=\"blue\"><em><b>Regression</b></em></font>: Given the value of a D-dimensional input vector $\\mathbf{x}$, predict the value of one or more <em>target</em> variables</p>\n",
    "# <p><font color=\"blue\"><b><em>Linear</em></b></font>: The models discussed in this section are <em>linear</em> with respect to the adjustable parameters, <em>not</em> \n",
    "#     necessisarily with respect to the input variables. </p>\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h1>Creating A Model</h1>\n",
    "# In this notebook, our objective is to construct models that can predict the value of some target variable, $t$, given some \n",
    "# input vector, $\\mathbf{x}$, where the target value can occupy any value in some space - though here we'll only consider the space of \n",
    "# real valued vectors. We want the models to allow for uncertainty in the accuracy of the model and/or noise on the observed data. \n",
    "# We also want the model to provide some information on our confidence in a given prediction. \n",
    "# \n",
    "# The first step is to contruct a mathematical model that adequately represents the observations we wish to predict. \n",
    "# The model we will use is described in the next two subsections. It is **important to note** that the model itself is independent \n",
    "# of the use of a frequentist or Bayesian viewpoint. It is *how we obtain the free parameters* of the model that is affected by using\n",
    "# frequentist or Bayesian approaches. However, if the model is a poor choice for a particular observation, then its predictive \n",
    "# capability is likely to be poor whether we use a frequentist or Bayesian approach to obtain the parameters.\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h2><font size=\"4\">Gaussian Noise: Model Assumption 1</font></h2>\n",
    "# We will *assume* throughout this notebook that the target variable is described by <br/><br/>\n",
    "#     $t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$\n",
    "#     <br/><br/>\n",
    "# where $y(\\mathbf{x},\\mathbf{w})$ is an as of yet undefined function of $\\mathbf{x}$ and $\\mathbf{w}$ and $\\epsilon$ is a <font color=\"red\"><em>Gaussian</em></font> distributed noise component. \n",
    "# \n",
    "# **Gaussian Noise?** The derivations provided below all assume Gaussian noise on the target data. Is this a good assumption? In many cases yes. The argument hinges\n",
    "# on the use of the [Central_Limit_Theorem](http://en.wikipedia.org/wiki/Central_limit_theorem) that basically says the the **sum** of many independent random\n",
    "# variables behaves behaves like a Gaussian distributed random variable. The _noise_ term in this model, $\\epsilon$, can be thought of as the sum of features\n",
    "# not included in the model function, $y(\\mathbf{x},\\mathbf{w})$. Assuming these features are themselves independent random variables then the Central Limit Theorom suggests a Gaussian model \n",
    "# is appropriate, assuming there are many independent unaccounted for features. It is possible that there is only a small number of unaccounted for features\n",
    "# or that there is genuine _non-Gauisian_ noise in our observation measurements, e.g. sensor shot noise that often has a Poisson distribution. In such cases, the assumption is no longer valid.\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h2><font size=\"4\">General Linear Model: Model Assumption 2</font></h2>\n",
    "# In order to proceed, we need to define a model for $y(\\mathbf{x},\\mathbf{w})$. We will use the *general linear regression* model defined as follows <br/><br/>\n",
    "#     $y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})$ <br/><br/>\n",
    "#     where $\\mathbf{x}$ is a $D$ dimensional input vector, $M$ is the number of free parameters in the model, $\\mathbf{w}$ is a column \n",
    "# vector of the free parameters, and \n",
    "# $\\phi(\\mathbf{x}) = \\\\{\\phi_0(\\mathbf{x}),\\phi_1(\\mathbf{x}), \\ldots,\\phi_{M-1}(\\mathbf{x})\\\\}$ with $\\phi_0(\\mathbf{x})=1$ is a set of basis functions where \n",
    "#     each $\\phi_i$ is in the real valued function space \n",
    "#     $\\\\{f \\in \\mathbf{R}^D\\Rightarrow\\mathbf{R}^1\\\\}$. It is important to note that the set of basis functions, $\\phi$, <font color=\"red\">need\n",
    "#     not be linear</font> with respect to $\\mathbf{x}$. Further, note that this model defines an entire class of models. In order to \n",
    "#     contruct an actual predictive model for some observable quantity, we will have to make a further assumption on the choice of the\n",
    "#     set of basis functions, $\\phi$. However, for the purposes of deriving general results, we can delay this choice.\n",
    "# \n",
    "# Note that that $\\mathbf{w}^T$ is an $1 \\times M$ vector and that $\\mathbf{\\phi}(\\mathbf{x})$ is a $M \\times 1$ vector so that the target, $y$ \n",
    "#     is a scalar. This will be exteneded to $K$ dimensional target variables below.\n",
    "# \n",
    "#     \n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h1>Frequentist View: Maximum Likelihood</h1>\n",
    "# Let's now embark on the path of obtaining the free parameters, $\\mathbf{w}$, of our model. We will begin using a *frequentist*, or \n",
    "# *maximum likelihood*, approach. This approach assumes that we first obtain observation training data, $\\mathbf{t}$, and that the *best* \n",
    "# value of $\\mathbf{w}$, is that which maximizes the likelihood function, $p(\\mathbf{t}|\\mathbf{w})$.\n",
    "# \n",
    "# <p>Under the Gaussian noise condition it can be shown that the maximum likelihood function for the training data is <br/><br/>\n",
    "#     \n",
    "#     $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\sigma^2) = \\prod_{n=1}^N ND(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\sigma^2)$ <br/><br/>\n",
    "#     \n",
    "#     $=\\frac{N}{2}\\ln\\frac{1}{\\sigma^2} -\\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N\n",
    "#     \\{t_n -\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2$ <br/><br/>\n",
    "#     \n",
    "#     where $\\mathbf{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\}$ is the input value set for the corresponding $N$ oberved output values contained in the vector \n",
    "#     $\\mathbf{t}$, and $ND(\\mu,\\sigma^2)$ is the Normal Distribution (Gaussian). (I used ND instead of the standard N to avoid confusion \n",
    "#     with the product limit).\n",
    "#     \n",
    "#     Taking the logarithm of the maximum likelihood and setting the derivative with respect to $\\mathbf{w}$ equal to zero, one can obtain \n",
    "#     the maximum likelikhood parameters given by the <em>normal equations</em>: <br/><br/>\n",
    "#     $\\mathbf{w}_{ML} = \\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$ <br/><br/>\n",
    "#     where $\\Phi$ is the $N \\times M$ <em>design matrix</em> with elements $\\Phi_{n,j}=\\phi_j(\\mathbf{x}_n)$, and $\\mathbf{t}$ is the $N \\times K$\n",
    "#     matrix of training set target values (for $K=1$, it is simply a column vector). Note that $\\mathbf{\\Phi}^T$ is a $M \\times N$ matrix, so that $\\mathbf{w}_{ML}=\\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$ is \n",
    "# $(M \\times N)\\times(N \\times M)\\times(M\\times N)\\times(N \\times K) = M \\times K$, where $M$ is the number of free parameters and $K$ is the number of predicted \n",
    "# target values for a given input. <br/>\n",
    "# </p>\n",
    "# \n",
    "# Note that the only term in the likelihood function that depends on $\\mathbf{w}$ is the last term. <font color=\"red\">Thus, maximizing the likelihood\n",
    "# function with respect to $\\mathbf{w}$ __under the assumption of Gaussian noise__ is equivalent to minimizing a \n",
    "# sum-of-squares error function. </font>\n",
    "# \n",
    "# <p>\n",
    "#     The quantity, $\\mathbf{\\Phi}^\\dagger=\\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T$ is known as the \n",
    "#     <em>Moore-Penrose pseudo-inverse</em> of $\\Phi$. When $\\Phi^T\\Phi$ is invertible, the pseudo-inverse is \n",
    "#     equivalent to the inverse. When this condition fails, the pseudo-inverse can be found with techniques such as <em>singular value decomposition</em>.\n",
    "# </p>\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# <h3>Example 1</h3>\n",
    "# <h4>(a) Linear Data</h4>\n",
    "# <p>Let's generate data of the for $y = m*x + b + \\epsilon $ where $\\epsilon$ is a random Gaussian component with zero mean. Given this data, let's apply the maximum likelihood \n",
    "#     solution to find values for the parameters $m$ and $b$. Given that we know our data is linear, we chose basis functions $\\phi_0(x)=1$ and $\\phi_1(x)=x$. Thus, our \n",
    "#     our model will be $y=\\theta_0\\phi_0(x) + \\theta_1\\phi_1(x)$, where presumabely the solution should yield $\\theta_0 \\approx b$ and $\\theta_1 \\approx\n",
    "#     m$\n",
    "# </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
